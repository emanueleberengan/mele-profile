
# meleâ€‘profile

This space is where I develop my skills, experiment with new technologies, and keep my knowledge up to date.  
Youâ€™ll find learning notebooks, handsâ€‘on LLM/GPT experiments, and a short profile.

---

## Quick links

- â¡ï¸ **Profile** â€” background, CVâ€‘style info, and contacts: **[profile/README.md*
- ğŸ¤– **GPT** â€” GPTâ€‘style training notebooks and AI experiments: **GPT/**

---

## Whatâ€™s in `/profile`

- **Overview & summary** of my experience (presales, cloud/edge, retail & banking).
- **Skills & certifications** (Azure/GCP, LLMs, observability, leadership).
- **Selected projects** with brief context and outcomes.
- **Contact** details.

ğŸ‘‰ Open: **[rofile/README.md**

---

## Whatâ€™s in `/GPT`

A practical sandbox for Large Language Models, focused on **training from scratch** and understanding learning dynamics on small corpora.

**Table of contents (TOC):**
- **`intro-training.ipynb`** â€” first principles:
  - loading text & GPTâ€‘2 tokenization  
  - 90/10 trainâ€“test split  
  - dataloaders for nextâ€‘token prediction  
  - crossâ€‘entropy loss  
  - a minimal training loop
- **`training.ipynb`** â€” deeper dive:
  - AdamW loop with periodic evaluation  
  - tracking training vs test loss  
  - controlled **overfitting** on a tiny dataset  
  - text generation samples  
  - loss curve visualization

ğŸ‘‰ Open the folder: **GPT/**

<details>
  <summary>Why no repository tree here?</summary>

Large code blocks with ASCII trees can overflow and hurt readability on small screens.  
This README keeps sections short and links directly to folders instead.
</details>

---

## Notes

