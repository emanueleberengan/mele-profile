{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Intro: Loading GPT‑2 Variants & What the Config Knobs Change\n",
    "\n",
    "This notebook downloads OpenAI’s GPT‑2 weights and loads them into a compatible PyTorch model to generate text.  \n",
    "You can switch among GPT‑2 sizes (Small/Medium/Large/XL) by changing the **model configuration** you pass to `GPTModel` and by downloading the matching **pretrained weights**.\n",
    "\n",
    "Below is a quick guide to what each config field does, how it affects **downloading weights** and **text generation**, and a few practical tips.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File already exists and is up-to-date: gpt2\\124M\\checkpoint\n",
      "File already exists and is up-to-date: gpt2\\124M\\encoder.json\n",
      "File already exists and is up-to-date: gpt2\\124M\\hparams.json\n",
      "File already exists and is up-to-date: gpt2\\124M\\model.ckpt.data-00000-of-00001\n",
      "File already exists and is up-to-date: gpt2\\124M\\model.ckpt.index\n",
      "File already exists and is up-to-date: gpt2\\124M\\model.ckpt.meta\n",
      "File already exists and is up-to-date: gpt2\\124M\\vocab.bpe\n",
      "Settings {'n_vocab': 50257, 'n_ctx': 1024, 'n_embd': 768, 'n_head': 12, 'n_layer': 12}\n",
      "Params keys dict_keys(['blocks', 'b', 'g', 'wpe', 'wte'])\n"
     ]
    }
   ],
   "source": [
    "from gpt_download import download_and_load_gpt2\n",
    "from gpt2 import GPT_CONFIG_124M, GPTModel, text_to_token_ids, generate_t_k, token_ids_to_text\n",
    "import torch, tiktoken\n",
    "import numpy as np\n",
    "\n",
    "settings, params = download_and_load_gpt2(\n",
    "    model_size=\"124M\", models_dir=\"gpt2\"\n",
    ")\n",
    "\n",
    "print(\"Settings\", settings)\n",
    "print(\"Params keys\",params.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transfer from Tensorflow to GPT implementation Q, K and V including Bias "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assign(left, right):\n",
    "    if left.shape != right.shape:\n",
    "        raise ValueError(f\"Shape mismatch. Left: {left.shape}, Right: {right.shape}\")\n",
    "    return torch.nn.Parameter(torch.tensor(right))\n",
    "\n",
    "\n",
    "def load_weights_into_gpt(gpt, params):\n",
    "    gpt.pos_emb.weight = assign(gpt.pos_emb.weight, params[\"wpe\"])\n",
    "    gpt.token_emb.weight = assign(gpt.token_emb.weight, params[\"wte\"])\n",
    "\n",
    "    for b in range(len(params[\"blocks\"])):\n",
    "        q_w, k_w, v_w = np.split(\n",
    "            (params[\"blocks\"][b][\"attn\"][\"c_attn\"])[\"w\"], 3, axis=-1)\n",
    "        gpt.trf_blocks[b].att.W_query.weight = assign(\n",
    "            gpt.trf_blocks[b].att.W_query.weight, q_w.T)\n",
    "        gpt.trf_blocks[b].att.W_key.weight = assign(\n",
    "            gpt.trf_blocks[b].att.W_key.weight, k_w.T)\n",
    "        gpt.trf_blocks[b].att.W_value.weight = assign(\n",
    "            gpt.trf_blocks[b].att.W_value.weight, v_w.T)\n",
    "\n",
    "        q_b, k_b, v_b = np.split(\n",
    "            (params[\"blocks\"][b][\"attn\"][\"c_attn\"])[\"b\"], 3, axis=-1)\n",
    "        gpt.trf_blocks[b].att.W_query.bias = assign(\n",
    "            gpt.trf_blocks[b].att.W_query.bias, q_b)\n",
    "        gpt.trf_blocks[b].att.W_key.bias = assign(\n",
    "            gpt.trf_blocks[b].att.W_key.bias, k_b)\n",
    "        gpt.trf_blocks[b].att.W_value.bias = assign(\n",
    "            gpt.trf_blocks[b].att.W_value.bias, v_b)\n",
    "\n",
    "        gpt.trf_blocks[b].att.out_proj.weight = assign(\n",
    "            gpt.trf_blocks[b].att.out_proj.weight,\n",
    "            params[\"blocks\"][b][\"attn\"][\"c_proj\"][\"w\"].T)\n",
    "        gpt.trf_blocks[b].att.out_proj.bias = assign(\n",
    "            gpt.trf_blocks[b].att.out_proj.bias,\n",
    "            params[\"blocks\"][b][\"attn\"][\"c_proj\"][\"b\"])\n",
    "\n",
    "        gpt.trf_blocks[b].ff.layers[0].weight = assign(\n",
    "            gpt.trf_blocks[b].ff.layers[0].weight,\n",
    "            params[\"blocks\"][b][\"mlp\"][\"c_fc\"][\"w\"].T)\n",
    "        gpt.trf_blocks[b].ff.layers[0].bias = assign(\n",
    "            gpt.trf_blocks[b].ff.layers[0].bias,\n",
    "            params[\"blocks\"][b][\"mlp\"][\"c_fc\"][\"b\"])\n",
    "        gpt.trf_blocks[b].ff.layers[2].weight = assign(\n",
    "            gpt.trf_blocks[b].ff.layers[2].weight,\n",
    "            params[\"blocks\"][b][\"mlp\"][\"c_proj\"][\"w\"].T)\n",
    "        gpt.trf_blocks[b].ff.layers[2].bias = assign(\n",
    "            gpt.trf_blocks[b].ff.layers[2].bias,\n",
    "            params[\"blocks\"][b][\"mlp\"][\"c_proj\"][\"b\"])\n",
    "\n",
    "        gpt.trf_blocks[b].norm1.scale = assign(\n",
    "            gpt.trf_blocks[b].norm1.scale,\n",
    "            params[\"blocks\"][b][\"ln_1\"][\"g\"])\n",
    "        gpt.trf_blocks[b].norm1.shift = assign(\n",
    "            gpt.trf_blocks[b].norm1.shift,\n",
    "            params[\"blocks\"][b][\"ln_1\"][\"b\"])\n",
    "        gpt.trf_blocks[b].norm2.scale = assign(\n",
    "            gpt.trf_blocks[b].norm2.scale,\n",
    "            params[\"blocks\"][b][\"ln_2\"][\"g\"])\n",
    "        gpt.trf_blocks[b].norm2.shift = assign(\n",
    "            gpt.trf_blocks[b].norm2.shift,\n",
    "            params[\"blocks\"][b][\"ln_2\"][\"b\"])\n",
    "\n",
    "    gpt.final_norm.scale = assign(gpt.final_norm.scale, params[\"g\"])\n",
    "    gpt.final_norm.shift = assign(gpt.final_norm.shift, params[\"b\"])\n",
    "    gpt.out_head.weight = assign(gpt.out_head.weight, params[\"wte\"])\n",
    "    \n",
    "\n",
    "model_configs = {\n",
    "    \"gpt2-small (124M)\": { \"emb_dim\" : 768, \"n_layers\" : 12, \"n_heads\" : 12},\n",
    "    \"gpt2-medium (355M)\" : {\"emb_dim\" : 1024, \"n_layers\" : 24, \"n_heads\" : 16},\n",
    "    \"gpt2-large (774M)\" : {\"emb_dim\" : 1280, \"n_layers\" : 36, \"n_heads\" : 20},\n",
    "    \"gpt2-xl (1558M)\" : {\"emb_dim\" : 1600, \"n_layers\" : 48, \"n_heads\" : 25}\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### The `model_configs` knobs (and what they impact)\n",
    "\n",
    "- **`emb_dim` (hidden size)**\n",
    "  - **What it is:** Width of token/hidden representations.\n",
    "  - **Download impact:** Must match the width of the downloaded checkpoint. If it doesn’t, weight loading will fail due to shape mismatches.\n",
    "  - **Generation impact:** Larger = typically better fluency/knowledge, but more GPU/CPU memory and slower inference.\n",
    "\n",
    "- **`n_layers` (number of transformer blocks)**\n",
    "  - **What it is:** Depth of the model (stacked transformer blocks).\n",
    "  - **Download impact:** Must match the checkpoint’s depth; otherwise weights won’t align with your model’s layers.\n",
    "  - **Generation impact:** Deeper = generally better quality/longer-range reasoning, but slower and more memory‑hungry.\n",
    "\n",
    "- **`n_heads` (attention heads per layer)**\n",
    "  - **What it is:** Parallel attention subspaces; must evenly divide `emb_dim`.\n",
    "  - **Download impact:** Must match the checkpoint; mismatches cause shape errors when splitting/merging Q/K/V projections.\n",
    "  - **Generation impact:** More heads (with matching `emb_dim`) improve attention expressivity; compute cost grows accordingly.\n",
    "\n",
    "- **`context_length` (a.k.a. sequence length / block size)**\n",
    "  - **What it is:** Maximum tokens the model attends to at once.\n",
    "  - **Download impact:** Checkpoints are trained for a certain context window (GPT‑2 was trained for 1024). You can **set a larger number**, but the weights aren’t trained for it—generation beyond the trained window may degrade.\n",
    "  - **Generation impact:** Higher = can consider longer prompts but uses more memory and can slow down attention quadratically with sequence length.\n",
    "\n",
    "- **`qkv_bias` (bias terms in Q/K/V projections)**\n",
    "  - **What it is:** Whether linear projections for Q/K/V include bias parameters.\n",
    "  - **Download impact:** Must match the original architecture of the checkpoint. If the checkpoint has no bias but your model expects it (or vice versa), shapes won’t match.\n",
    "  - **Generation impact:** Minor quality/speed effect compared to other knobs; mainly matters for weight compatibility.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"gpt2-small (124M)\"\n",
    "NEW_CONFIG = GPT_CONFIG_124M.copy()\n",
    "NEW_CONFIG.update(model_configs[model_name])\n",
    "NEW_CONFIG.update({\"context_length\":1024})\n",
    "NEW_CONFIG.update({\"qkv_bias\":True})\n",
    "\n",
    "print(\"NEW_CONFIG\",NEW_CONFIG)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "gpt = GPTModel(NEW_CONFIG)\n",
    "gpt.eval()\n",
    "\n",
    "load_weights_into_gpt(gpt,params)\n",
    "gpt.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Practical trade‑offs\n",
    "\n",
    "- **Quality vs. speed/memory**\n",
    "  - **Small (124M)**: Fastest, least memory, good for quick tests.\n",
    "  - **Medium/Large/XL**: Better generations, but progressively heavier and slower.\n",
    "- **Context window**\n",
    "  - Keep `context_length = 1024` for faithful GPT‑2 behavior. Larger values are possible but not trained, may degrade beyond 1024 tokens and will increase compute.\n",
    "- **Sampling controls**\n",
    "  - `temperature` and `top_k` shape the creativity and diversity of outputs:\n",
    "    - Higher `temperature` → more diverse/creative (riskier).\n",
    "    - Lower `top_k` → safer/more focused; higher → more variety.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated text gpt2 style: \n",
      "\n",
      " El Nino es desaparecido es el niente, en suo noche, con una hacienda, un poblaciones y que esta que es están de sus suis.<|endoftext|>The first thing you'll notice when you see this image, which was released just after the end of the 2016 season, was this massive, red blob of red, which is actually a blue jellyfish. It appears to float like it did on an orange ocean liner as we were filming the opening sequences. And\n"
     ]
    }
   ],
   "source": [
    "#torch.manual_seed(123)\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "token_ids = generate_t_k(\n",
    "    model=gpt,\n",
    "    idx=text_to_token_ids(\"Il ragazzo è sparito\",tokenizer=tokenizer).to(device),\n",
    "    max_new_tokens=100,\n",
    "    context_size=NEW_CONFIG[\"context_length\"],\n",
    "    top_k=10,\n",
    "    temperature=1.5\n",
    ")\n",
    "\n",
    "print(\"Generated text gpt2 style: \\n\\n\", token_ids_to_text(token_ids=token_ids,tokenizer=tokenizer))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
