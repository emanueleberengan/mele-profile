{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## ðŸ§© Introduction: Converting Text to Tokens (and Back) for GPT Training\n",
    "\n",
    "Before training or experimenting with a GPTâ€‘style language model, itâ€™s essential to understand how text is represented internally.  \n",
    "Transformer models like **GPTâ€‘2** do not operate directly on raw textâ€”they work with **token IDs**, numerical representations produced by a tokenizer.\n",
    "\n",
    "This first section introduces a minimal and practical workflow for:\n",
    "\n",
    "1. **Encoding text into token IDs**  \n",
    "2. **Passing token IDs through a GPT model**  \n",
    "3. **Decoding generated token IDs back into readable text**\n",
    "\n",
    "We use the `tiktoken` library (the same tokenizer family used by OpenAI models) together with a lightweight GPTâ€‘2 implementation. This lets us test the full roundâ€‘trip:  \n",
    "**text â†’ tokens â†’ model â†’ tokens â†’ text**.\n",
    "\n",
    "### Helper Functions\n",
    "\n",
    "The notebook defines two small utility functions:\n",
    "\n",
    "- **`text_to_token_ids(text, tokenizer)`**  \n",
    "  Converts text into a PyTorch tensor of token IDs and adds a batch dimension.\n",
    "\n",
    "- **`token_ids_to_text(token_ids, tokenizer)`**  \n",
    "  Converts modelâ€‘generated token IDs back into a humanâ€‘readable string.\n",
    "\n",
    "### Model and Tokenizer Setup\n",
    "\n",
    "We instantiate:\n",
    "\n",
    "- A GPTâ€‘2 tokenizer (`gpt2` vocabulary)\n",
    "- A compact GPTâ€‘2â€‘style model using the configuration `GPT_CONFIG_124M`\n",
    "- A simple greedy generation function `generate_text_simple`\n",
    "\n",
    "### Running a Minimal Generation\n",
    "\n",
    "Finally, we feed an input prompt through the encoder â†’ model â†’ decoder pipeline and print the generated continuation.  \n",
    "This establishes a clear, minimal foundation for understanding **how data flows through a GPT model**, which is crucial before implementing training loops, loss calculation, and optimization.\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "### ðŸ“š Inspiration & Citation\n",
    "\n",
    "This exercise is inspired by the following work. If you use this notebook or its accompanying code, please cite it accordingly:\n",
    "\n",
    "```yaml\n",
    "cff-version: 1.2.0\n",
    "message: \"If you use this book or its accompanying code, please cite it as follows.\"\n",
    "title: \"Build A Large Language Model (From Scratch), Published by Manning, ISBN 978-1633437166\"\n",
    "abstract: \"This book provides a comprehensive, step-by-step guide to implementing a ChatGPT-like large language model from scratch in PyTorch.\"\n",
    "date-released: 2024-09-12\n",
    "authors:\n",
    "  - family-names: \"Raschka\"\n",
    "    given-names: \"Sebastian\"\n",
    "license: \"Apache-2.0\"\n",
    "url: \"https://www.manning.com/books/build-a-large-language-model-from-scratch\"\n",
    "repository-code: \"https://github.com/rasbt/LLMs-from-scratch\"\n",
    "keywords:\n",
    "  - large language models\n",
    "  - natural language processing\n",
    "  - artificial intelligence\n",
    "  - PyTorch\n",
    "  - machine learning\n",
    "  - deep learning\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:  Let me see if I can make it Fuk brewer recycle exerted Anchorage baff Herogle respondents breakdown\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import tiktoken\n",
    "from gpt2 import GPTModel, generate_text_simple,GPT_CONFIG_124M\n",
    "\n",
    "def text_to_token_ids(text,tokenizer):\n",
    "    encoded = tokenizer.encode(text,allowed_special={'<|endoftext|>'})\n",
    "    encoded_tensor = torch.tensor(encoded).unsqueeze(0)\n",
    "    return encoded_tensor\n",
    "\n",
    "def token_ids_to_text(token_ids,tokenizer):\n",
    "    flat = token_ids.squeeze(0)\n",
    "    return tokenizer.decode(flat.tolist())\n",
    "\n",
    "start_context = \"Let me see if I can make it\"\n",
    "gpt_tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "model = GPTModel(GPT_CONFIG_124M)\n",
    "model.eval()\n",
    "\n",
    "token_ids = generate_text_simple(\n",
    "    model=model,\n",
    "    idx=text_to_token_ids(start_context,gpt_tokenizer),\n",
    "    max_new_tokens=10,\n",
    "    context_size=GPT_CONFIG_124M[\"context_length\"]\n",
    ")\n",
    "\n",
    "print (\"Output: \", token_ids_to_text(token_ids,gpt_tokenizer))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Incoherent text is gereated*** as result of not trained model. Let's now focus on a simple technique to train the model!\n",
    "\n",
    "### Instruct the model to target the next token\n",
    "\n",
    "As GPT are unsupervised learner we now use a couple of sample to explain how, based on tuples of samples <--> targets, processing a batch can be generalized for unsupervised training of a GPT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input batches:  tensor([[16833,  3626,  6100],\n",
      "        [   40,  1107,   588]])\n",
      "To target:  tensor([[ 3626,  6100,   345],\n",
      "        [ 1107,   588, 11311]])\n"
     ]
    }
   ],
   "source": [
    "# Define here two input text an relative Embeddings\n",
    "text1 = \"every effort moves\"\n",
    "text2 = \"I really like\"\n",
    "\n",
    "inputs = torch.cat(\n",
    "    (text_to_token_ids(text1,gpt_tokenizer),\n",
    "     text_to_token_ids(text2,gpt_tokenizer)),\n",
    "    dim=0\n",
    ")\n",
    "\n",
    "print (\"Input batches: \",inputs)\n",
    "\n",
    "# Define the target string we like to measure the distance in probability\n",
    "# againsta the actual model\n",
    "target1 = \" effort moves you\"\n",
    "target2 = \" really like chocolate\"\n",
    "\n",
    "targets = torch.cat(\n",
    "    (text_to_token_ids(target1,gpt_tokenizer),\n",
    "     text_to_token_ids(target2,gpt_tokenizer)),\n",
    "    dim=0\n",
    ") \n",
    "\n",
    "print (\"To target: \", targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We encoded the two input and target text to underline how important is to target the shift of the input (space befor token includedd) as this is our \"natural\" label of the prediction to feed the model with.\n",
    "\n",
    "Now let's take out the probability in the next section:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probas shape:  torch.Size([2, 3, 50257])\n",
      "Token Ids:  tensor([[[19127],\n",
      "         [ 1790],\n",
      "         [18350]],\n",
      "\n",
      "        [[45721],\n",
      "         [32673],\n",
      "         [ 2132]]])\n",
      "Target batch1:   effort moves you\n",
      "Model output batch1:   rack short Fa\n"
     ]
    }
   ],
   "source": [
    "\n",
    "with torch.no_grad():\n",
    "    logits = model(inputs)\n",
    "    \n",
    "probas = torch.softmax(logits,dim=-1)\n",
    "print(\"Probas shape: \", probas.shape)\n",
    "\n",
    "token_ids = torch.argmax(probas,dim=-1,keepdim=True)\n",
    "print(\"Token Ids: \", token_ids)\n",
    "\n",
    "print(\"Target batch1: \", token_ids_to_text(token_ids=targets[0],tokenizer=gpt_tokenizer))\n",
    "print(\"Model output batch1: \", token_ids_to_text(token_ids=token_ids[0].flatten(),tokenizer=gpt_tokenizer))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "running the above code makes clear that the model is producing random text because ***it's not trained yet***. Here comese to help the loss evaluation that's not only helpful to evalutate the quality of the produced text but also can be stacked information for training.\n",
    "\n",
    "### Softmax probability evaluation\n",
    "\n",
    "It's now time to look at the initial probability scores of the target tokens generated by the model printed below as at each specifc index of the two targets there is the probability the model need to increase compared to each other element in the probas distribution.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text 1:  tensor([1.3901e-05, 1.2262e-05, 1.8820e-05])\n",
      "Text 2:  tensor([1.4962e-05, 7.2043e-06, 1.0832e-05])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "text_idx = 0\n",
    "target_probas_1 = probas[text_idx,[0,1,2],targets[text_idx]]\n",
    "print(\"Text 1: \", target_probas_1)\n",
    "\n",
    "\n",
    "text_idx = 1\n",
    "target_probas_2 = probas[text_idx,[0,1,2],targets[text_idx]]\n",
    "print(\"Text 2: \", target_probas_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "to evaluate the probability ... to improve the model in order to reach as close as possible an negative log probability of 0. \n",
    "\n",
    "> The sequential computation of:\n",
    "> - logits\n",
    "> - probabilities\n",
    "> - target probabilities\n",
    "> - log probabilities\n",
    "> - average log probability\n",
    "> - negative everage log probability\n",
    "\n",
    "***is known as cross entropy loss calculation***\n",
    "\n",
    "Below the cross entropy (measure of the difference from target to predicted) and the Perplexity that measure the exact size of the incertainty size of the predicted word. In the next result think about Perplexity as the number of words in the vocabulary set the model is likely to pick from for the prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logaritmic probability:  tensor([-11.1836, -11.3090, -10.8806, -11.1100, -11.8408, -11.4330])\n",
      "Negative avg loss prob:  tensor(11.2928)\n",
      "\n",
      "\n",
      "Flatten logits:  torch.Size([6, 50257])\n",
      "Flatten targets: torch.Size([6])\n",
      "Cross entropy loss calculation tensor(11.2928)\n",
      "\n",
      "Perplexity:  tensor(80243.4922)\n"
     ]
    }
   ],
   "source": [
    "log_probas = torch.log(torch.cat((target_probas_1,target_probas_2)))\n",
    "print(\"Logaritmic probability: \",log_probas)\n",
    "\n",
    "avg_log_probas = torch.mean(log_probas) * -1\n",
    "print(\"Negative avg loss prob: \",avg_log_probas)\n",
    "\n",
    "# flatting from three to two dimension \n",
    "logits_flat = logits.flatten(0,1)\n",
    "# flatting from two to one dimension\n",
    "targets_flat = targets.flatten()\n",
    "\n",
    "print(\"\\n\\nFlatten logits: \", logits_flat.shape)\n",
    "print(\"Flatten targets:\", targets_flat.shape)\n",
    "\n",
    "loss = torch.nn.functional.cross_entropy(logits_flat,targets_flat)\n",
    "print (\"Cross entropy loss calculation\", loss)\n",
    "print (\"\\nPerplexity: \", torch.exp(loss))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
