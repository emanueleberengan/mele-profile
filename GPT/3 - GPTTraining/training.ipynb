{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## TODO\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "### ðŸ“š Inspiration & Citation\n",
    "\n",
    "This exercise is inspired by the following work. If you use this notebook or its accompanying code, please cite it accordingly:\n",
    "\n",
    "```yaml\n",
    "cff-version: 1.2.0\n",
    "message: \"If you use this book or its accompanying code, please cite it as follows.\"\n",
    "title: \"Build A Large Language Model (From Scratch), Published by Manning, ISBN 978-1633437166\"\n",
    "abstract: \"This book provides a comprehensive, step-by-step guide to implementing a ChatGPT-like large language model from scratch in PyTorch.\"\n",
    "date-released: 2024-09-12\n",
    "authors:\n",
    "  - family-names: \"Raschka\"\n",
    "    given-names: \"Sebastian\"\n",
    "license: \"Apache-2.0\"\n",
    "url: \"https://www.manning.com/books/build-a-large-language-model-from-scratch\"\n",
    "repository-code: \"https://github.com/rasbt/LLMs-from-scratch\"\n",
    "keywords:\n",
    "  - large language models\n",
    "  - natural language processing\n",
    "  - artificial intelligence\n",
    "  - PyTorch\n",
    "  - machine learning\n",
    "  - deep learning\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import the text_data for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Characters:  20479\n",
      "Tokens:  5145\n"
     ]
    }
   ],
   "source": [
    "import os, tiktoken, torch\n",
    "from gpt2 import create_dataloader_v1, GPT_CONFIG_124M\n",
    "\n",
    "file_path = \"the-verdict.txt\" # Free book from Edith Wharton (inspired by credits)\n",
    "text_data = \"\"\n",
    "with open(file_path,\"r\",encoding=\"utf-8\") as file:\n",
    "    text_data = file.read()\n",
    "    \n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "    \n",
    "total_char = len(text_data)\n",
    "total_tokens = len(tokenizer.encode(text_data))\n",
    "\n",
    "print (\"Characters: \", total_char)\n",
    "print (\"Tokens: \", total_tokens)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and prepare the data in training and validation sets splitting the data with the below train_ratio of 90% training 10% test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "invalid literal for int() with base 10: 'I'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 8\u001b[0m\n\u001b[0;32m      4\u001b[0m test_data \u001b[38;5;241m=\u001b[39m text_data[split_idx:]\n\u001b[0;32m      6\u001b[0m torch\u001b[38;5;241m.\u001b[39mmanual_seed(\u001b[38;5;241m234\u001b[39m)\n\u001b[1;32m----> 8\u001b[0m train_loader \u001b[38;5;241m=\u001b[39m \u001b[43mcreate_dataloader_v1\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mGPT_CONFIG_124M\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcontext_length\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstride\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mGPT_CONFIG_124M\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcontext_length\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     13\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdrop_last\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     14\u001b[0m \u001b[43m    \u001b[49m\u001b[43mshuffle\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     15\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_workers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\n\u001b[0;32m     16\u001b[0m \u001b[43m)\u001b[49m\n\u001b[0;32m     18\u001b[0m test_loader \u001b[38;5;241m=\u001b[39m create_dataloader_v1(\n\u001b[0;32m     19\u001b[0m     test_data,\n\u001b[0;32m     20\u001b[0m     batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     25\u001b[0m     num_workers\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m\n\u001b[0;32m     26\u001b[0m )\n",
      "File \u001b[1;32mc:\\emanuele\\mele-profile\\GPT\\3 - GPTTraining\\gpt2.py:216\u001b[0m, in \u001b[0;36mcreate_dataloader_v1\u001b[1;34m(txt, batch_size, max_length, stride, shuffle, drop_last, num_workers)\u001b[0m\n\u001b[0;32m    213\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    215\u001b[0m \u001b[38;5;66;03m# Create dataset\u001b[39;00m\n\u001b[1;32m--> 216\u001b[0m dataset \u001b[38;5;241m=\u001b[39m \u001b[43mGPTDatasetV1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtxt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstride\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    218\u001b[0m \u001b[38;5;66;03m# Create dataloader\u001b[39;00m\n\u001b[0;32m    219\u001b[0m dataloader \u001b[38;5;241m=\u001b[39m DataLoader(\n\u001b[0;32m    220\u001b[0m     dataset,\n\u001b[0;32m    221\u001b[0m     batch_size\u001b[38;5;241m=\u001b[39mbatch_size,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    224\u001b[0m     num_workers\u001b[38;5;241m=\u001b[39mnum_workers\n\u001b[0;32m    225\u001b[0m )\n",
      "File \u001b[1;32mc:\\emanuele\\mele-profile\\GPT\\3 - GPTTraining\\gpt2.py:194\u001b[0m, in \u001b[0;36mGPTDatasetV1.__init__\u001b[1;34m(self, txt, tokenizer, max_length, stride)\u001b[0m\n\u001b[0;32m    190\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_ids \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m    192\u001b[0m \u001b[38;5;66;03m# Modification\u001b[39;00m\n\u001b[0;32m    193\u001b[0m \u001b[38;5;66;03m# token_ids = tokenizer.encode(txt, allowed_special={\"<|endoftext|>\"})\u001b[39;00m\n\u001b[1;32m--> 194\u001b[0m token_ids \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mint\u001b[39m(i) \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m txt\u001b[38;5;241m.\u001b[39mstrip()\u001b[38;5;241m.\u001b[39msplit()]\n\u001b[0;32m    196\u001b[0m \u001b[38;5;66;03m# Use a sliding window to chunk the book into overlapping sequences of max_length\u001b[39;00m\n\u001b[0;32m    197\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;28mlen\u001b[39m(token_ids) \u001b[38;5;241m-\u001b[39m max_length, stride):\n",
      "File \u001b[1;32mc:\\emanuele\\mele-profile\\GPT\\3 - GPTTraining\\gpt2.py:194\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    190\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_ids \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m    192\u001b[0m \u001b[38;5;66;03m# Modification\u001b[39;00m\n\u001b[0;32m    193\u001b[0m \u001b[38;5;66;03m# token_ids = tokenizer.encode(txt, allowed_special={\"<|endoftext|>\"})\u001b[39;00m\n\u001b[1;32m--> 194\u001b[0m token_ids \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mi\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m txt\u001b[38;5;241m.\u001b[39mstrip()\u001b[38;5;241m.\u001b[39msplit()]\n\u001b[0;32m    196\u001b[0m \u001b[38;5;66;03m# Use a sliding window to chunk the book into overlapping sequences of max_length\u001b[39;00m\n\u001b[0;32m    197\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;28mlen\u001b[39m(token_ids) \u001b[38;5;241m-\u001b[39m max_length, stride):\n",
      "\u001b[1;31mValueError\u001b[0m: invalid literal for int() with base 10: 'I'"
     ]
    }
   ],
   "source": [
    "train_ratio = 0.90\n",
    "split_idx = int(train_ratio * len(text_data))\n",
    "train_data = text_data[:split_idx]\n",
    "test_data = text_data[split_idx:]\n",
    "\n",
    "torch.manual_seed(234)\n",
    "\n",
    "train_loader = create_dataloader_v1(\n",
    "    train_data,\n",
    "    batch_size=2,\n",
    "    max_length=GPT_CONFIG_124M[\"context_length\"],\n",
    "    stride=GPT_CONFIG_124M[\"context_length\"],\n",
    "    drop_last=True,\n",
    "    shuffle=True,\n",
    "    num_workers=0\n",
    ")\n",
    "\n",
    "test_loader = create_dataloader_v1(\n",
    "    test_data,\n",
    "    batch_size=2,\n",
    "    max_length=GPT_CONFIG_124M[\"context_length\"],\n",
    "    stride=GPT_CONFIG_124M[\"context_length\"],\n",
    "    drop_last=False,\n",
    "    shuffle=False,\n",
    "    num_workers=0\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We encoded the two input and target text to underline how important is to target the shift of the input (space befor token includedd) as this is our \"natural\" label of the prediction to feed the model with.\n",
    "\n",
    "Now let's take out the probability in the next section:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probas shape:  torch.Size([2, 3, 50257])\n",
      "Token Ids:  tensor([[[19127],\n",
      "         [ 1790],\n",
      "         [18350]],\n",
      "\n",
      "        [[45721],\n",
      "         [32673],\n",
      "         [ 2132]]])\n",
      "Target batch1:   effort moves you\n",
      "Model output batch1:   rack short Fa\n"
     ]
    }
   ],
   "source": [
    "\n",
    "with torch.no_grad():\n",
    "    logits = model(inputs)\n",
    "    \n",
    "probas = torch.softmax(logits,dim=-1)\n",
    "print(\"Probas shape: \", probas.shape)\n",
    "\n",
    "token_ids = torch.argmax(probas,dim=-1,keepdim=True)\n",
    "print(\"Token Ids: \", token_ids)\n",
    "\n",
    "print(\"Target batch1: \", token_ids_to_text(token_ids=targets[0],tokenizer=gpt_tokenizer))\n",
    "print(\"Model output batch1: \", token_ids_to_text(token_ids=token_ids[0].flatten(),tokenizer=gpt_tokenizer))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "running the above code makes clear that the model is producing random text because ***it's not trained yet***. Here comese to help the loss evaluation that's not only helpful to evalutate the quality of the produced text but also can be stacked information for training.\n",
    "\n",
    "### Softmax probability evaluation\n",
    "\n",
    "It's now time to look at the initial probability scores of the target tokens generated by the model printed below as at each specifc index of the two targets there is the probability the model need to increase compared to each other element in the probas distribution.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text 1:  tensor([1.3901e-05, 1.2262e-05, 1.8820e-05])\n",
      "Text 2:  tensor([1.4962e-05, 7.2043e-06, 1.0832e-05])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "text_idx = 0\n",
    "target_probas_1 = probas[text_idx,[0,1,2],targets[text_idx]]\n",
    "print(\"Text 1: \", target_probas_1)\n",
    "\n",
    "\n",
    "text_idx = 1\n",
    "target_probas_2 = probas[text_idx,[0,1,2],targets[text_idx]]\n",
    "print(\"Text 2: \", target_probas_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "to evaluate the probability ... to improve the model in order to reach as close as possible an negative log probability of 0. \n",
    "\n",
    "> The sequential computation of:\n",
    "> - logits\n",
    "> - probabilities\n",
    "> - target probabilities\n",
    "> - log probabilities\n",
    "> - average log probability\n",
    "> - negative everage log probability\n",
    "\n",
    "***is known as cross entropy loss calculation***\n",
    "\n",
    "Below the cross entropy (measure of the difference from target to predicted) and the Perplexity that measure the exact size of the incertainty size of the predicted word. In the next result think about Perplexity as the number of words in the vocabulary set the model is likely to pick from for the prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logaritmic probability:  tensor([-11.1836, -11.3090, -10.8806, -11.1100, -11.8408, -11.4330])\n",
      "Negative avg loss prob:  tensor(11.2928)\n",
      "\n",
      "\n",
      "Flatten logits:  torch.Size([6, 50257])\n",
      "Flatten targets: torch.Size([6])\n",
      "Cross entropy loss calculation tensor(11.2928)\n",
      "\n",
      "Perplexity:  tensor(80243.4922)\n"
     ]
    }
   ],
   "source": [
    "log_probas = torch.log(torch.cat((target_probas_1,target_probas_2)))\n",
    "print(\"Logaritmic probability: \",log_probas)\n",
    "\n",
    "avg_log_probas = torch.mean(log_probas) * -1\n",
    "print(\"Negative avg loss prob: \",avg_log_probas)\n",
    "\n",
    "# flatting from three to two dimension \n",
    "logits_flat = logits.flatten(0,1)\n",
    "# flatting from two to one dimension\n",
    "targets_flat = targets.flatten()\n",
    "\n",
    "print(\"\\n\\nFlatten logits: \", logits_flat.shape)\n",
    "print(\"Flatten targets:\", targets_flat.shape)\n",
    "\n",
    "loss = torch.nn.functional.cross_entropy(logits_flat,targets_flat)\n",
    "print (\"Cross entropy loss calculation\", loss)\n",
    "print (\"\\nPerplexity: \", torch.exp(loss))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
