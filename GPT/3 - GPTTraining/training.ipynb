{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## ![Pear](https://cdn.pixabay.com) vsüçé Simple training on language coherence\n",
    "\n",
    "In this sample book inspired by the citation I will use a short book to check learning on a GPT model from scratch. As the input is not at significant scale to be able to generalize a language model we will experience a constant learning rate path vs a test loss overfit.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import the text_data for training\n",
    "\n",
    "A gpt2 file is collected within the folder for an easy import of code from previous chapters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, tiktoken, torch\n",
    "from gpt2 import GPTModel,create_dataloader_v1, GPT_CONFIG_124M, generate_text_simple\n",
    "\n",
    "file_path = \"the-verdict.txt\" # Free book from Edith Wharton (inspired by credits)\n",
    "text_data = \"\"\n",
    "with open(file_path,\"r\",encoding=\"utf-8\") as file:\n",
    "    text_data = file.read()\n",
    "    \n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "    \n",
    "total_char = len(text_data)\n",
    "total_tokens = len(tokenizer.encode(text_data))\n",
    "\n",
    "print (\"Characters: \", total_char)\n",
    "print (\"Tokens: \", total_tokens)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and prepare the data in training and validation sets splitting the data with the below train_ratio of 90% training 10% test. We use here the create_dataloader_v1 to prepare the imput data in batches of 256 tokens each.\n",
    "\n",
    "The train_loader and test_loader will be used in the following code as \"cursors\" to train the model with the usual cylcle evaluation of epochs and losses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ratio = 0.90\n",
    "split_idx = int(train_ratio * len(text_data))\n",
    "train_data = text_data[:split_idx]\n",
    "test_data = text_data[split_idx:]\n",
    "\n",
    "torch.manual_seed(234)\n",
    "\n",
    "train_loader = create_dataloader_v1(\n",
    "    train_data,\n",
    "    batch_size=2,\n",
    "    max_length=GPT_CONFIG_124M[\"context_length\"],\n",
    "    stride=GPT_CONFIG_124M[\"context_length\"],\n",
    "    drop_last=True,\n",
    "    shuffle=True,\n",
    "    num_workers=0\n",
    ")\n",
    "\n",
    "test_loader = create_dataloader_v1(\n",
    "    test_data,\n",
    "    batch_size=2,\n",
    "    max_length=GPT_CONFIG_124M[\"context_length\"],\n",
    "    stride=GPT_CONFIG_124M[\"context_length\"],\n",
    "    drop_last=False,\n",
    "    shuffle=False,\n",
    "    num_workers=0\n",
    ")\n",
    "\n",
    "print (\"Train loader: \\n\")\n",
    "for x,y in train_loader:\n",
    "    print(x.shape,y.shape)\n",
    "    \n",
    "print (\"Test loader: \\n\")\n",
    "for x,y in test_loader:\n",
    "    print(x.shape,y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the üçêvsüçé.. loss\n",
    "\n",
    "It's now time to focus on loss. As we have seen in the intro-training.ipynb loss can be calculated across the batches with the cross entropy function. In the next two functions we will calculate loss for input and target batches and iterate over the bateches in a dataloader to accumulate loss and return the avg loss for the number of processed batched along the iteration:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a basic calculation of loss using the cross entropy function from torch\n",
    "def calc_loss_batch(input_batch, target_batch, model, device):\n",
    "    input_batch = input_batch.to(device)\n",
    "    target_batch = target_batch.to(device)\n",
    "    \n",
    "    logits = model(input_batch)\n",
    "    loss = torch.nn.functional.cross_entropy(\n",
    "        logits.flatten(0,1), target_batch.flatten() # Cross entropy fucntion applies to flatten batches\n",
    "    )\n",
    "    return loss\n",
    "\n",
    "def calc_loss_dataloaders(data_loader, model, device, num_batches=None):\n",
    "    total_loss = 0\n",
    "    if len(data_loader) == 0:\n",
    "        return float(\"nan\")\n",
    "    elif num_batches is None:\n",
    "        num_batches = len(data_loader)\n",
    "    else:\n",
    "        num_batches = min (num_batches, len(data_loader))\n",
    "    \n",
    "    for i, (input_batch, target_batch) in enumerate (data_loader):\n",
    "        if i < num_batches:\n",
    "            loss = calc_loss_batch(\n",
    "                input_batch=input_batch,target_batch=target_batch,model=model,device=device\n",
    "            )\n",
    "            total_loss += loss.item()\n",
    "        else:\n",
    "            break\n",
    "    return total_loss/num_batches\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model with a symple cycle üö≤\n",
    "\n",
    "#### In the following section we are going to implement a typical training over epoques:\n",
    "\n",
    "Whyle epoques:\n",
    "> - Train the model --> model.train()\n",
    "> - While you have trainings batches:\n",
    ">> - restart your gradient calculations --> optimizer.zerograd()\n",
    ">> - go for loss calc and backpropagate --> learn over the training samples\n",
    ">> - make one step forward --> optimizer.setp()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_to_token_ids(text,tokenizer):\n",
    "    encoded = tokenizer.encode(text,allowed_special={'<|endoftext|>'})\n",
    "    encoded_tensor = torch.tensor(encoded).unsqueeze(0)\n",
    "    return encoded_tensor\n",
    "\n",
    "\n",
    "def token_ids_to_text(token_ids,tokenizer):\n",
    "    flat = token_ids.squeeze(0)\n",
    "    return tokenizer.decode(flat.tolist())\n",
    "\n",
    "def evaluate_model( model, train_loader, test_loader, device, eval_iter):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        train_loss = calc_loss_dataloaders(\n",
    "            train_loader,model,device, num_batches=eval_iter\n",
    "        )\n",
    "        test_loss = calc_loss_dataloaders(\n",
    "            test_loader,model,device, num_batches=eval_iter\n",
    "        )\n",
    "    model.train()\n",
    "    return train_loss, test_loss\n",
    "\n",
    "def generate_and_print_samples( model, tokenizer, device, start_context):\n",
    "    model.eval()\n",
    "    context_size = model.pos_emb.weight.shape[0]\n",
    "    encoded = text_to_token_ids(start_context,tokenizer).to(device)\n",
    "    with torch.no_grad():\n",
    "        token_ids = generate_text_simple(model=model,idx=encoded,max_new_tokens=50,context_size=context_size)\n",
    "    decoded_text = token_ids_to_text(token_ids,tokenizer)\n",
    "    print(\"decoded text: \", decoded_text.replace(\"\\n\",\" \"))\n",
    "\n",
    "def train_model_simple(model, train_loader, test_loader, optimizer, device, num_epochs, eval_freq, eval_iter, start_context, tokenizer):\n",
    "    train_losses, test_losses, track_token_seen = [], [], []\n",
    "    token_seen, global_step = 0, -1\n",
    "    \n",
    "    # epoch cycle\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train() # put the model in training mode for the backpropagation\n",
    "        # and start cycling over input/target batches from the training dataloader\n",
    "        for input_batch, target_batch in train_loader:\n",
    "            \n",
    "            optimizer.zero_grad() # at the beginning of each batch it's suggested to nullify the gradients and restart from scratch\n",
    "            \n",
    "            # get loss for the current batch\n",
    "            loss = calc_loss_batch(\n",
    "                input_batch,target_batch,model, device\n",
    "                \n",
    "            )\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            token_seen += input_batch.numel() # + one more in the stack of seen tokens\n",
    "            \n",
    "            global_step += 1\n",
    "            \n",
    "            # Just to print out the current status of training for the model object we are working with\n",
    "            if global_step % eval_freq == 0:\n",
    "                train_loss, test_loss = evaluate_model(\n",
    "                    model, train_loader, test_loader, device, eval_iter\n",
    "                )\n",
    "                train_losses.append(train_loss)\n",
    "                test_losses.append(test_loss)\n",
    "                track_token_seen.append(token_seen)\n",
    "                print   (\n",
    "                    f\"Ep {epoch+1} (Step: {global_step:06d}): \"\n",
    "                    f\"Train loss {train_loss:.3f}, \"\n",
    "                    f\"Test loss {test_loss:.3f}\"\n",
    "                )\n",
    "        \n",
    "        # let's print out some text samples to check the aility of the model to generate coherent text        \n",
    "        generate_and_print_samples(\n",
    "            model, tokenizer, device, start_context\n",
    "        )\n",
    "    \n",
    "    return train_losses, test_losses, track_token_seen\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ‚ñ∂Ô∏è See the model training in action\n",
    "\n",
    "let's now start the training that will take 5 minutes over an I7 CPU and see it the introduction sentence of this book is valid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "torch.manual_seed(123)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = GPTModel(GPT_CONFIG_124M)\n",
    "model.to(device)\n",
    "\n",
    "optimizer = torch.optim.AdamW(\n",
    "    model.parameters(),\n",
    "    lr=0.0004,weight_decay=0.1\n",
    ")\n",
    "\n",
    "num_epochs = 10\n",
    "train_losses, test_losses, token_seen = train_model_simple(\n",
    "    model=model,train_loader=train_loader,test_loader=test_loader,optimizer=optimizer,device=device,num_epochs=num_epochs,\n",
    "    eval_freq=5,\n",
    "    eval_iter=5,\n",
    "    start_context=\"Every effort moves you\",\n",
    "    tokenizer=tokenizer\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot restults in to visualize learn vs test and underline again that, because of the very limited dataset, the model is learning but overfiting and stop (really reduce) it's power of generalization at epoch number 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "\n",
    "def plot_losses(epochs_seen, tokens_seen, train_losses, test_losses, ):\n",
    "    fig, ax1 = plt.subplots(figsize = (5,3))\n",
    "    ax1.plot(epochs_seen,train_losses,label=\"Training loss\")\n",
    "    ax1.plot(epochs_seen,test_losses,linestyle=\"-.\",label=\"Test loss\")\n",
    "    \n",
    "    ax1.set_xlabel(\"Epoch\")\n",
    "    ax1.set_ylabel(\"Loss\")\n",
    "    ax1.legend(loc=\"upper right\")\n",
    "    ax1.xaxis.set_major_locator(MaxNLocator(integer=True))\n",
    "    ax2 = ax1.twiny()\n",
    "    ax2.plot(tokens_seen,train_losses,alpha=0)\n",
    "    ax2.set_xlabel(\"Token seen\")\n",
    "    fig.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "\n",
    "epoch_tensor = torch.linspace(0,num_epochs,len(train_losses))\n",
    "plot_losses(epoch_tensor,token_seen,train_losses,test_losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### üìö Inspiration & Citation\n",
    "\n",
    "This exercise is inspired by the following work. If you use this notebook or its accompanying code, please cite it accordingly:\n",
    "\n",
    "```yaml\n",
    "cff-version: 1.2.0\n",
    "message: \"If you use this book or its accompanying code, please cite it as follows.\"\n",
    "title: \"Build A Large Language Model (From Scratch), Published by Manning, ISBN 978-1633437166\"\n",
    "abstract: \"This book provides a comprehensive, step-by-step guide to implementing a ChatGPT-like large language model from scratch in PyTorch.\"\n",
    "date-released: 2024-09-12\n",
    "authors:\n",
    "  - family-names: \"Raschka\"\n",
    "    given-names: \"Sebastian\"\n",
    "license: \"Apache-2.0\"\n",
    "url: \"https://www.manning.com/books/build-a-large-language-model-from-scratch\"\n",
    "repository-code: \"https://github.com/rasbt/LLMs-from-scratch\"\n",
    "keywords:\n",
    "  - large language models\n",
    "  - natural language processing\n",
    "  - artificial intelligence\n",
    "  - PyTorch\n",
    "  - machine learning\n",
    "  - deep learning"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
