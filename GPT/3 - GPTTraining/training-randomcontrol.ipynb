{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### ü´é Increase the creativity with random controll\n",
    "\n",
    "Explore the different techniques to controll randomness generation of text with the aim to: conserve the ability to generate grammatically correct text but increase variance.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Just build the model\n",
    "\n",
    "A collapsed version of [training.ipynb](training.ipynb) that rebuilds the model. \n",
    "\n",
    "After the first run, the model is saved as a `.pth` file in the current directory and automatically reloaded in all subsequent runs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken, torch, os\n",
    "from gpt2 import GPTModel,create_dataloader_v1, GPT_CONFIG_124M, generate_text_simple\n",
    "from tcc import train_model_simple, text_to_token_ids, token_ids_to_text\n",
    "\n",
    "saved_model = \"the-verdict.pth\"\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = GPTModel(GPT_CONFIG_124M)\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "if not os.path.isfile(saved_model):\n",
    "    file_path = \"the-verdict.txt\" # Free book from Edith Wharton (inspired by credits)\n",
    "    text_data = \"\"\n",
    "    with open(file_path,\"r\",encoding=\"utf-8\") as file:\n",
    "        text_data = file.read()\n",
    "        \n",
    "    \n",
    "        \n",
    "    train_ratio = 0.90\n",
    "    split_idx = int(train_ratio * len(text_data))\n",
    "    train_data = text_data[:split_idx]\n",
    "    test_data = text_data[split_idx:]\n",
    "\n",
    "    torch.manual_seed(234)\n",
    "\n",
    "    train_loader = create_dataloader_v1(\n",
    "        train_data,\n",
    "        batch_size=2,\n",
    "        max_length=GPT_CONFIG_124M[\"context_length\"],\n",
    "        stride=GPT_CONFIG_124M[\"context_length\"],\n",
    "        drop_last=True,\n",
    "        shuffle=True,\n",
    "        num_workers=0\n",
    "    )\n",
    "\n",
    "    test_loader = create_dataloader_v1(\n",
    "        test_data,\n",
    "        batch_size=2,\n",
    "        max_length=GPT_CONFIG_124M[\"context_length\"],\n",
    "        stride=GPT_CONFIG_124M[\"context_length\"],\n",
    "        drop_last=False,\n",
    "        shuffle=False,\n",
    "        num_workers=0\n",
    "    )\n",
    "\n",
    "    torch.manual_seed(123)\n",
    "    \n",
    "    model.to(device)\n",
    "\n",
    "    optimizer = torch.optim.AdamW(\n",
    "        model.parameters(),\n",
    "        lr=0.0004,weight_decay=0.1\n",
    "    )\n",
    "\n",
    "    num_epochs = 10\n",
    "    train_losses, test_losses, token_seen = train_model_simple(\n",
    "        model=model,train_loader=train_loader,test_loader=test_loader,optimizer=optimizer,device=device,num_epochs=num_epochs,\n",
    "        eval_freq=5,\n",
    "        eval_iter=5,\n",
    "        start_context=\"Every effort moves you\",\n",
    "        tokenizer=tokenizer\n",
    "    )\n",
    "    torch.save(\n",
    "        {\n",
    "        \"model_state_dict\": model.state_dict(),\n",
    "        \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "        },\n",
    "        saved_model\n",
    "    )\n",
    "else:\n",
    "    model.load_state_dict(torch.load(saved_model,map_location=device)[\"model_state_dict\"])\n",
    "    model.eval()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### ‚öóÔ∏è Working with Temperature and Top‚ÄëK Sampling\n",
    "\n",
    "The following code demonstrates how different sampling techniques affect a model‚Äôs ability to generate coherent, context‚Äëaware, and syntactically correct text.  \n",
    "Experiment with each parameter to observe changes in style, creativity, and stability.\n",
    "\n",
    "---\n",
    "\n",
    "#### üî• Temperature  \n",
    "Temperature controls randomness in token selection:\n",
    "\n",
    "- **0.0 ‚Üí 0.7** ‚Äî deterministic and safe  \n",
    "- **0.7 ‚Üí 1.0** ‚Äî balanced creativity  \n",
    "- **1.0 ‚Üí 1.5** ‚Äî high diversity, risk of losing coherence  \n",
    "\n",
    "#### üî¢ Top‚ÄëK Sampling  \n",
    "Top‚ÄëK restricts sampling to the **K highest‚Äëprobability tokens**:\n",
    "\n",
    "- **K = 3‚Äì5** keeps context while allowing variation  \n",
    "- Higher values increase creativity but may destabilize meaning\n",
    "\n",
    "#### üßµ End‚Äëof‚ÄëString (EOS)  \n",
    "An EOS token ID enables the model to stop generation cleanly at a semantic boundary.\n",
    "\n",
    "#### üìè Max New Tokens  \n",
    "Controls output length:\n",
    "\n",
    "- Lower values ‚Üí short completions  \n",
    "- Higher values ‚Üí long-form generation\n",
    "\n",
    "#### üõ†Ô∏è Implementation Notes\n",
    "\n",
    "- `torch.topk()` keeps only the highest-logit options  \n",
    "- Masking with `float('-inf')` removes unwanted tokens from sampling  \n",
    "- Softmax converts masked logits into **0 probability**  \n",
    "- `torch.multinomial()` introduces controlled randomness instead of picking the highest value every time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output text: \n",
      " Every effort moves you?\"\n",
      "Yes, and pushed one of the deep arm-chairs forward. I could have given Miss Croft the fullest reass\n"
     ]
    }
   ],
   "source": [
    "def generate_t_k(model,idx,max_new_tokens,context_size, temperature=0.0,top_k=None,eos_id=None):\n",
    "    for _ in range(max_new_tokens):\n",
    "        idx_cond = idx[:,-context_size:]\n",
    "        with torch.no_grad():\n",
    "            logits = model(idx_cond)\n",
    "            \n",
    "        logits = logits[:,-1,:]\n",
    "        if top_k is not None:\n",
    "            top_logits, _ = torch.topk(logits,top_k)\n",
    "            min_val = top_logits[:,-1]\n",
    "            \n",
    "            logits = torch.where(\n",
    "                logits < min_val,\n",
    "                torch.tensor(float('-inf')).to(logits.device),\n",
    "                logits\n",
    "            )\n",
    "            \n",
    "        if temperature > 0.0 :\n",
    "            logits = logits / temperature\n",
    "            probs = torch.softmax(logits,-1)\n",
    "            idx_next = torch.multinomial(probs,num_samples=1)\n",
    "        if idx_next == eos_id:\n",
    "            break\n",
    "        \n",
    "        idx = torch.cat((idx,idx_next),-1)\n",
    "        \n",
    "    return idx\n",
    "\n",
    "token_ids = generate_t_k(\n",
    "    model=model,\n",
    "    idx=text_to_token_ids(\"Every effort moves you\", tokenizer),\n",
    "    max_new_tokens=25,\n",
    "    context_size=GPT_CONFIG_124M[\"context_length\"],\n",
    "    top_k = 5,\n",
    "    temperature=1\n",
    "    #,eos_id=text_to_token_ids('.',tokenizer)\n",
    ")\n",
    "\n",
    "print(\"Output text: \\n\",token_ids_to_text(token_ids,tokenizer))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### üìö Inspiration & Citation\n",
    "\n",
    "This exercise is inspired by the following work. If you use this notebook or its accompanying code, please cite it accordingly:\n",
    "\n",
    "```yaml\n",
    "cff-version: 1.2.0\n",
    "message: \"If you use this book or its accompanying code, please cite it as follows.\"\n",
    "title: \"Build A Large Language Model (From Scratch), Published by Manning, ISBN 978-1633437166\"\n",
    "abstract: \"This book provides a comprehensive, step-by-step guide to implementing a ChatGPT-like large language model from scratch in PyTorch.\"\n",
    "date-released: 2024-09-12\n",
    "authors:\n",
    "  - family-names: \"Raschka\"\n",
    "    given-names: \"Sebastian\"\n",
    "license: \"Apache-2.0\"\n",
    "url: \"https://www.manning.com/books/build-a-large-language-model-from-scratch\"\n",
    "repository-code: \"https://github.com/rasbt/LLMs-from-scratch\"\n",
    "keywords:\n",
    "  - large language models\n",
    "  - natural language processing\n",
    "  - artificial intelligence\n",
    "  - PyTorch\n",
    "  - machine learning\n",
    "  - deep learning"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
